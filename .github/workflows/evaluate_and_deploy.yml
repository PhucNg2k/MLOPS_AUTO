name: Evaluate and Deploy Model

on:
  push:
    branches:
      - dev
    paths:
      - 'data/models/final/**'  # Catch any changes in models directory
      - 'src/**'                # Keep watching source code changes
  workflow_dispatch:  # Allow manual trigger
    inputs:
      reason:
        description: 'Reason for manual trigger'
        required: false
        default: 'Manual evaluation requested'

permissions:
  contents: write
  issues: write

jobs:
  evaluate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
        ref: dev
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create directories
      run: |
        mkdir -p validation_data evaluation_results
        chmod -R 755 validation_data evaluation_results
        
    - name: Find latest model and download from S3
      id: find_model
      run: |
        # Find latest DVC file first
        latest_dvc=$(ls -t data/models/final/model_*.pth.dvc 2>/dev/null | head -n1)
        if [ -z "$latest_dvc" ]; then
          echo "No DVC file found in data/models/final directory"
          exit 1
        fi
        
        # Get model filename from DVC file name
        model_filename=$(basename "${latest_dvc%.dvc}")
        echo "Found latest model DVC file: $latest_dvc for model: $model_filename"
        
        # Download model file directly from S3
        aws s3 cp "${{ secrets.DVC_REMOTE_URL }}/$model_filename" "data/models/final/$model_filename"
        
        if [ ! -f "data/models/final/$model_filename" ]; then
          echo "Failed to download model file from S3"
          exit 1
        fi
        
        echo "model_path=data/models/final/$model_filename" >> $GITHUB_OUTPUT
        echo "Found and downloaded latest model: $model_filename"
    
    - name: Download validation dataset
      run: |
        # Remove s3:// prefix if present
        BUCKET_PATH="${{ secrets.VALIDATE_BUCKET }}"
        BUCKET_NAME=${BUCKET_PATH#"s3://"}
        
        # Download validation dataset from bucket root
        aws s3 sync s3://$BUCKET_NAME validation_data/
        chmod -R 755 validation_data
        
        # Verify required files exist
        if [ ! -e "./validation_data/images" ] || [ ! -e "./validation_data/image_labels.json" ] || [ ! -e "./validation_data/validation_set_stats.json" ]; then
          echo "Error: Required validation files missing"
          echo "Contents of validation_data:"
          ls -la ./validation_data/
          exit 1
        fi
        
        echo "Successfully downloaded validation dataset with required files"
    
    - name: Run evaluation
      id: evaluate
      run: |
        python src/evaluate.py \
          --model_path ${{ steps.find_model.outputs.model_path }} \
          --data_dir validation_data \
          --output_dir evaluation_results \
          --threshold 85.0
          
        echo "metrics=$(cat evaluation_results/eval_metrics.json)" >> $GITHUB_OUTPUT
    
    - name: Check evaluation results
      id: check_eval
      run: |
        metrics='${{ steps.evaluate.outputs.metrics }}'
        current_accuracy=$(echo $metrics | python -c "import sys, json; print(json.load(sys.stdin)['test_accuracy'])")
        threshold=85.0
        
        echo "Current accuracy: $current_accuracy%"
        echo "Required threshold: $threshold%"
        
        if (( $(echo "$current_accuracy >= $threshold" | bc -l) )); then
          echo "merge=true" >> $GITHUB_OUTPUT
          echo "Model meets accuracy threshold!"
        else
          echo "merge=false" >> $GITHUB_OUTPUT
          echo "Model did not meet accuracy threshold"
        fi
    
    - name: Deploy to production if passed
      if: steps.check_eval.outputs.merge == 'true'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --global user.email "github-actions@github.com"
        git config --global user.name "GitHub Actions"
        
        # Prepare what to merge to main - only .dvc files and evaluation results
        git checkout -b temp-merge
        
        # Remove everything except .dvc files and evaluation results
        git rm -rf .
        git checkout dev -- data/models/final/*.dvc evaluation_results/
        
        # Commit the cleaned state
        git commit -m "Clean state for main branch"
        
        # Switch to main and merge the clean state
        git checkout main
        git pull origin main
        git merge --no-ff temp-merge -m "Merge validated model to production

        Evaluation Results:
        - Accuracy: $(echo ${{ steps.evaluate.outputs.metrics }} | python -c 'import sys,json; print(json.load(sys.stdin)["test_accuracy"])')%
        - Status: PASSED
        - Model: $(basename ${{ steps.find_model.outputs.model_path }})"
        
        # Push to main
        git push origin main
        
        # Cleanup temp branch
        git branch -D temp-merge
    
    - name: Create issue if failed
      if: steps.check_eval.outputs.merge == 'false'
      uses: actions/github-script@v4
      with:
        script: |
          github.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Model Evaluation Failed',
            body: `Model did not meet accuracy threshold (85%)
            
            Evaluation Results:
            ${process.env.METRICS}
            
            To reproduce:
            1. Checkout this commit
            2. See .dvc file for model version`,
            labels: ['model-evaluation', 'failed']
          })
      env:
        METRICS: ${{ steps.evaluate.outputs.metrics }} 